{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cira\n",
    "\n",
    "cira.auth.KEY_FILE = \"../../alpc_key.json\"\n",
    "assert cira.auth.check_keys(), \"the set keys dose not work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = cira.Portfolio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "assets_symbols = [\"MSFT\", \"TSLA\", \"AMZN\"]\n",
    "stk_hist_data = {}\n",
    "IS_CACHED = True\n",
    "\n",
    "for SYMBOL in assets_symbols:\n",
    "    SYM_HIST_FILE = f\"./{SYMBOL}.csv\"\n",
    "\n",
    "    stk = cira.Stock(SYMBOL)\n",
    "\n",
    "    if not IS_CACHED:\n",
    "        start = datetime(2015, 7, 1)\n",
    "        end = datetime(2023, 7, 1)\n",
    "        stk.save_historical_data(SYM_HIST_FILE, start, end)\n",
    "    \n",
    "    data = stk.load_historical_data(SYM_HIST_FILE)\n",
    "    stk_hist_data[SYMBOL] = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSFT</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>CASH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04 05:00:00+00:00</th>\n",
       "      <td>54.80</td>\n",
       "      <td>223.41</td>\n",
       "      <td>636.99</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05 05:00:00+00:00</th>\n",
       "      <td>55.05</td>\n",
       "      <td>223.43</td>\n",
       "      <td>633.79</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06 05:00:00+00:00</th>\n",
       "      <td>54.05</td>\n",
       "      <td>219.04</td>\n",
       "      <td>632.65</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07 05:00:00+00:00</th>\n",
       "      <td>52.17</td>\n",
       "      <td>215.65</td>\n",
       "      <td>607.94</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08 05:00:00+00:00</th>\n",
       "      <td>52.33</td>\n",
       "      <td>211.00</td>\n",
       "      <td>607.05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-26 04:00:00+00:00</th>\n",
       "      <td>328.60</td>\n",
       "      <td>241.05</td>\n",
       "      <td>127.33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-27 04:00:00+00:00</th>\n",
       "      <td>334.57</td>\n",
       "      <td>250.21</td>\n",
       "      <td>129.18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-28 04:00:00+00:00</th>\n",
       "      <td>335.85</td>\n",
       "      <td>256.24</td>\n",
       "      <td>129.04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 04:00:00+00:00</th>\n",
       "      <td>335.05</td>\n",
       "      <td>257.50</td>\n",
       "      <td>127.90</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-30 04:00:00+00:00</th>\n",
       "      <td>340.43</td>\n",
       "      <td>261.77</td>\n",
       "      <td>130.36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1886 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             MSFT    TSLA    AMZN  CASH\n",
       "timestamp                                              \n",
       "2016-01-04 05:00:00+00:00   54.80  223.41  636.99   1.0\n",
       "2016-01-05 05:00:00+00:00   55.05  223.43  633.79   1.0\n",
       "2016-01-06 05:00:00+00:00   54.05  219.04  632.65   1.0\n",
       "2016-01-07 05:00:00+00:00   52.17  215.65  607.94   1.0\n",
       "2016-01-08 05:00:00+00:00   52.33  211.00  607.05   1.0\n",
       "...                           ...     ...     ...   ...\n",
       "2023-06-26 04:00:00+00:00  328.60  241.05  127.33   1.0\n",
       "2023-06-27 04:00:00+00:00  334.57  250.21  129.18   1.0\n",
       "2023-06-28 04:00:00+00:00  335.85  256.24  129.04   1.0\n",
       "2023-06-29 04:00:00+00:00  335.05  257.50  127.90   1.0\n",
       "2023-06-30 04:00:00+00:00  340.43  261.77  130.36   1.0\n",
       "\n",
       "[1886 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for sym, hist in stk_hist_data.items():\n",
    "    df[sym] = hist[\"close\"]\n",
    "\n",
    "df[\"CASH\"] = 1.0\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the Deep Q-Learning agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = []\n",
    "        self.model = QNetwork(state_size, action_size)\n",
    "        self.target_model = QNetwork(state_size, action_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(torch.Tensor(state))\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a random minibatch from the replay memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    target_q_values = self.target_model(torch.Tensor(next_state))\n",
    "                    target = reward + self.gamma * torch.max(target_q_values).item()\n",
    "\n",
    "            # Get the current Q-values\n",
    "            current_q_values = self.model(torch.Tensor(state))\n",
    "            target_q_values = current_q_values.clone()\n",
    "\n",
    "            # Update the Q-value for the chosen action\n",
    "            target_q_values[0][action] = target\n",
    "\n",
    "            # Compute the loss and backpropagate\n",
    "            loss = self.criterion(current_q_values, target_q_values)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Update target network every few episodes\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if self.epsilon < self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        if self.memory:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Example usage\n",
    "state_size = 4  # adjust based on your environment\n",
    "action_size = 2  # adjust based on your environment\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(1000):  # You may need more or fewer episodes\n",
    "    state = env.reset()  # Replace 'env' with your environment\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for time_step in range(500):  # Adjust the maximum time steps as needed\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        # Remember the experience and train the agent\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay(batch_size=32)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
